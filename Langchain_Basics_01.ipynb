{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M598_Iqc9b0F"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"\""
      ],
      "metadata": {
        "id": "3Jrlkp-3985T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdgNq2E1-QaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain Conditional Generation"
      ],
      "metadata": {
        "id": "qYX5RTM5AzQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using OpenAI GPT3"
      ],
      "metadata": {
        "id": "xiAYRkj3_num"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name = 'text-davinci-003',\n",
        "             temperature = 0.5,\n",
        "             max_tokens = 256)\n"
      ],
      "metadata": {
        "id": "-USXTnn8LvzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji3AISuuAcHX",
        "outputId": "39b5ecbd-8fa4-4da7-ab0e-e5c5aa6888a9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it was thirsty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8uYkWC_AvrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now using T5-Flan-XL"
      ],
      "metadata": {
        "id": "bAojFgBMA7Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "llm_hf = HuggingFaceHub(repo_id = \"google/flan-t5-xxl\", model_kwargs = {\"temperature\": 0.9, \"max_length\":512})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81rDn88JAvne",
        "outputId": "058a59a9-6eca-4686-a9a0-b768dd23e18d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "print(llm_hf(text, ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBhYaQ-rAvlc",
        "outputId": "ce9097c5-e098-42c8-f118-b7f7e588f63d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it wanted to get to the other side\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBlHUfg3AvjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates"
      ],
      "metadata": {
        "id": "hsqzNBcxB9lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "restaurant_template = \"\"\"\n",
        "I want you to act as a naming consultant for new restaurants.\n",
        "\n",
        "Return a list of 5 restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
        "\n",
        "What are some good names for a restaurant that is {restaurant_description}?\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = ['restaurant_description'],\n",
        "    template = restaurant_template\n",
        ")"
      ],
      "metadata": {
        "id": "XXeoCpm2Avhn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc_1 = \"A Indian Paneer cousin restaurant with musical fountain\"\n",
        "desc_2 = \"A Mallaio shop in the spiritual capital of India having ganga view\"\n",
        "desc_3 = \"a cafe that has live hard rock music and memorabilia\""
      ],
      "metadata": {
        "id": "rDGzk87CAvfe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(restaurant_description = desc_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "o5Pd42jUAvdB",
        "outputId": "92997018-6c56-4847-f029-9c401285ae3c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI want you to act as a naming consultant for new restaurants.\\n\\nReturn a list of 5 restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\\n\\nWhat are some good names for a restaurant that is A Indian Paneer cousin restaurant with musical fountain?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## querying the model with the prompt template\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(desc_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KQISeMfOsqx",
        "outputId": "3b453e67-53ff-452a-b713-6bf7bce0d699"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spice Island\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7mmzPiJOzdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Few Shot Learning"
      ],
      "metadata": {
        "id": "_6leABuASRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\n",
        "        \"word\": \"happy\",\n",
        "        \"antonym\": \"sad\"\n",
        "        },\n",
        "    {\n",
        "        \"word\": \"tall\",\n",
        "        \"antonym\": \"short\"\n",
        "        },\n",
        "]"
      ],
      "metadata": {
        "id": "QYDIByR-iFbn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we specify the template to format the examples we have provided.\n",
        "# We use the `PromptTemplate` class for this.\n",
        "example_formatter_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\\n\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_formatter_template,\n",
        ")"
      ],
      "metadata": {
        "id": "fCwBSGCiiKuX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we create the `FewShotPromptTemplate` object.\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    # These are the examples we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    # This is how we want to format the examples when we insert them into the prompt.\n",
        "    example_prompt=example_prompt,\n",
        "    # The prefix is some text that goes before the examples in the prompt.\n",
        "    # Usually, this consists of intructions.\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    # The suffix is some text that goes after the examples in the prompt.\n",
        "    # Usually, this is where the user input will go\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    # The input variables are the variables that the overall prompt expects.\n",
        "    input_variables=[\"input\"],\n",
        "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "RdokMUQjiWOc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We can now generate a prompt using the `format` method.\n",
        "print(few_shot_prompt.format(input=\"big\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUAtcPaciY5O",
        "outputId": "b63cc96d-5028-4074-f210-cd5697e1a4cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm_hf, prompt=few_shot_prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"Big\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g2lR51QibKv",
        "outputId": "c694f57a-cbf8-4275-e680-6bcfa4079425"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(input = \"Largest\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s0r8Xvzi9bv",
        "outputId": "986e7d0b-c279-4e93-ac3d-156e73474495"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smallest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"Rough\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLeCYIYVnVtg",
        "outputId": "cacfc617-f754-4952-fa7a-de0bae6d2c95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smooth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"Dark\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTs7K8anneVO",
        "outputId": "ec684131-e230-4b17-e6d5-9df88d22f61c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Light\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"Floor\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyakAvd_nh5m",
        "outputId": "a37ad876-ed60-4573-9895-37957ed2214e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceiling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5ERcR03npLM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}