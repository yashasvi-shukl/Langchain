{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nROhBwDtjp-C"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with Chains\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain — a `PromptTemplate` — to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain — a LLM.\n",
        "\n",
        "We'll start by importing all the libraries that we'll be using in this example."
      ],
      "metadata": {
        "id": "4HYfjBj5IxJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import inspect\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain, SimpleSequentialChain\n",
        "from langchain.callbacks import get_openai_callback"
      ],
      "metadata": {
        "id": "iNTd_QQCmCnB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "qviecFv5NQ5y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0)"
      ],
      "metadata": {
        "id": "5dI9pSdBNfyY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function will tell how many tokens we are using in each call\n",
        "\n",
        "def count_tokens(chain, query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result = chain.run(query)\n",
        "    print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "  return result"
      ],
      "metadata": {
        "id": "Af1_5sRjNsg5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Ot5etCxOkTH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why chains?\n",
        "In simple terms, a \"chain\" is like a series of steps in a process. These steps use different tools, like prompts or functions, to work on some input and give a result. There are three types of chains: **Utility chains, Generic chains, and Combine Documents chains**. We'll talk about Utility and Generic chains in this explanation.\n",
        "\n",
        "1.   Utility Chains are like ready-made tools that quickly get specific information from a source.\n",
        "2.   Generic Chains are like basic building blocks for making more complex tools, but you can't use them on their own.\n",
        "</br>\n",
        "\n",
        "Now, let's see what these chains can do!"
      ],
      "metadata": {
        "id": "GaKlveILQ7Il"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-8xJ84-RbOh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Chains\n",
        "\n",
        "1. LLMMathChain - Allows llms the ability to do maths."
      ],
      "metadata": {
        "id": "IYvX2U2ARg0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_math_chain = LLMMathChain.from_llm(\n",
        "    llm = llm,\n",
        "    verbose = True # to see the steps llm is taking to complete the task\n",
        ")"
      ],
      "metadata": {
        "id": "PAHqxSWqR2G_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(llm_math_chain, \"What is the 23 to the power of 0.456?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "5xs0RrqISQQH",
        "outputId": "392bfee2-d319-4828-a10f-a07235b853dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is the 23 to the power of 0.456?\u001b[32;1m\u001b[1;3m\n",
            "```text\n",
            "23**0.456\n",
            "```\n",
            "...numexpr.evaluate(\"23**0.456\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m4.17780238247165\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 265 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 4.17780238247165'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(llm_math_chain, \"What will be percentage of marks if someone secured 439 out of 500?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "ra7jUOxDS1IO",
        "outputId": "e74defbb-8960-436e-fcbd-c37aa34e38f0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What will be percentage of marks if someone secured 439 out of 500?\u001b[32;1m\u001b[1;3m\n",
            "```text\n",
            "(439/500)*100\n",
            "```\n",
            "...numexpr.evaluate(\"(439/500)*100\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m87.8\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 270 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 87.8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(llm_math_chain, \"A had 4 apples and 20 banana. The cost of a apple is twice of a banana. If the cost of whole apple is 200, then what is the cost of banana?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "c-4_muloU8_2",
        "outputId": "c4ce1798-8a5f-41c7-c969-231ee9e02e81"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "A had 4 apples and 20 banana. The cost of a apple is twice of a banana. If the cost of whole apple is 200, then what is the cost of banana?\u001b[32;1m\u001b[1;3m\n",
            "```text\n",
            "200 / (4 * 2)\n",
            "```\n",
            "...numexpr.evaluate(\"200 / (4 * 2)\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m25.0\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 293 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 25.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find out what's happening here. Someone asked a question using regular language, and the chain passed it on to the language model (llm). The llm then provided a Python code as the answer, which the chain used to give us the final answer. Now, you might wonder how the llm knew to give us Python code.\n",
        "\n",
        "Well, it's because of something called prompts. The question we send to the chain isn't the only thing the llm looks at. The question is put into a larger context, like a set of instructions, which helps the llm understand how to deal with our question. This set of instructions is called a prompt. Now, let's find out what specific prompt this chain is using!"
      ],
      "metadata": {
        "id": "O5xkFEy4Uo_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_math_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsdVCSDcTS7O",
        "outputId": "1cbe018b-ae37-4cbf-a254-6a24f813a7f9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kbunpw9lYL3X"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "okay, let's break it down. We're basically telling the computer not to solve difficult math problems by itself. Instead, it should show us a piece of Python code that can solve the math problem. If we didn't give this instruction, the computer might attempt to solve the problem on its own, and it might not succeed. Let's test this and see what happens! 🧐"
      ],
      "metadata": {
        "id": "i-bDPasfunom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables = ['question'], template = '{question}')\n",
        "llm_chain = LLMChain(prompt = prompt, llm = llm)\n",
        "\n",
        "\n",
        "count_tokens(llm_chain, \"What is 54 raised to the power of 0.689?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "jentKYvhvk6n",
        "outputId": "02285177-c502-4571-a806-a88755cf49f0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 18 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n2.945'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorrect response! </br>\n",
        "**Here's an important insight:** If we use prompts cleverly, we can make the language model (llm) avoid common mistakes by specifically and intentionally instructing it to act in a particular manner.\n",
        "\n",
        "Another cool thing about this process is that it doesn't just input information into the llm; it also puts together Python code. Let's take a closer look at how this process functions."
      ],
      "metadata": {
        "id": "rlWVkXfbxNXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inspect.getsource(llm_math_chain._call))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCWZTqdjx0FS",
        "outputId": "4f8b0996-ce7d-4689-a295-7216af5ed7c1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, str],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
            "        _run_manager.on_text(inputs[self.input_key])\n",
            "        llm_output = self.llm_chain.predict(\n",
            "            question=inputs[self.input_key],\n",
            "            stop=[\"```output\"],\n",
            "            callbacks=_run_manager.get_child(),\n",
            "        )\n",
            "        return self._process_llm_result(llm_output, _run_manager)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, if the llm gives us a simple math answer, we're good. But if it gives Python code, we use a **Python REPL simulator** to run and get the exact answer for harder problems. Cool!\n",
        "</br>\n",
        "</br>\n",
        "This is the chain process: llm gives an answer for easy math or Python code for tough problems, which we compile. Also, we see chain composition in action. We use `LLMMathChain`, which uses `LLMChain` when needed. We can make many such compositions, connecting chains for complex and customizable behavior.\n",
        "</br>\n",
        "</br>\n",
        "Utility chains follow a similar structure: a prompt to make llm give a specific response type. We can ask for SQL queries, API calls, or Bash commands on the go 🔥\n",
        "</br>\n",
        "</br>\n",
        "Langchain keeps growing, getting more flexible and powerful. Explore it and try the example notebooks.\n",
        "</br>\n",
        "</br>\n",
        "**A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line italicized text*"
      ],
      "metadata": {
        "id": "rKrAxPzk01qf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZ5TXz4-0xV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generic Chains\n",
        "\n",
        "In langchain, there are just three main types of chains, and we'll focus on demonstrating all of them in one example. Ready?\n",
        "</br></br>\n",
        "Imagine dealing with messy input texts. You see, language models charge us based on the number of tokens we use. We don't want to pay more just because our input has extra characters. It's not cool 😉\n",
        "</br></br>\n",
        "So, here's the plan: we'll create a special function to tidy up the spacing in our texts. Then, we'll use this function to make a chain. We'll feed in our messy text, and we want a nice, clean text as the result. Easy, right?"
      ],
      "metadata": {
        "id": "agz2TyhL1xku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_func(inputs: dict) -> dict:\n",
        "    text = inputs[\"text\"]\n",
        "\n",
        "    # replace multiple new lines and multiple spaces with a single one\n",
        "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "    return {\"output_text\": text}"
      ],
      "metadata": {
        "id": "rp1YJ7zt2ZOC"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Most importantly, when we start the chain, we don't include an \"llm\" as a parameter to it. Without this \"llm,\" the chain isn't as strong as the one we looked at before. But, as we'll see soon, if we combine this chain with others, we can get really good outcomes."
      ],
      "metadata": {
        "id": "TzrD8wOS8j6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_extra_spaces_chain = TransformChain(\n",
        "    input_variables = ['text'],\n",
        "    output_variables = ['output_text'],\n",
        "    transform = transform_func\n",
        ")\n",
        "\n",
        "clean_extra_spaces_chain.run(\"This is an exampe     of incorrect spacing and new line. \\n\\n\\n Let's see how chain works.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PyAbK-oR6URC",
        "outputId": "e7d772e5-31d8-4d85-c54d-220766b97ecf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This is an exampe of incorrect spacing and new line. \\n Let's see how chain works.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZUP325Yq9dtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Now, let's make things more fun.\n",
        "\n",
        "Imagine we want to take some text, clean it up, and then rewrite it in a special way, like making it sound poetic or like a police report. Since the `TransformChain` doesn't handle styling, we need another tool for that, and that's where our `LLMChain` comes into play. We've talked about this chain before, and we can do some cool stuff with clever prompts, so let's give it a try!\n",
        "\n",
        "First, we'll create the template for our prompt:"
      ],
      "metadata": {
        "id": "FA9vOQHg--4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Paraphrase this text:\n",
        "{output_text}\n",
        "In the style of a {style}.\n",
        "Paraphrase:  \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = ['style', 'output_text'],\n",
        "    template = template\n",
        ")"
      ],
      "metadata": {
        "id": "efL3xgug_CGL"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_paraphrase_chain = LLMChain(llm = llm, prompt = prompt, output_key = \"final_output\")"
      ],
      "metadata": {
        "id": "j_XrKIN0_sLQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! See how the input text in the template is named 'output_text'? Any idea why?\n",
        "\n",
        "We'll send what comes out of the `TransformChain` to the `LLMChain`!\n",
        "\n",
        "Last step, we have to join them together to act as a single chain. To do that, we'll use `SequentialChain`, our third basic building block for chains.\n",
        "\n",
        "Let's start by creating the prompt template:"
      ],
      "metadata": {
        "id": "rtbx1M0QATv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_chain = SequentialChain(\n",
        "    chains = [clean_extra_spaces_chain, style_paraphrase_chain],\n",
        "    input_variables = ['text', 'style'],\n",
        "    output_variables=['final_output']\n",
        "    )"
      ],
      "metadata": {
        "id": "-2AMvEYIASSM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "Chains allow us to combine multiple\n",
        "\n",
        "\n",
        "components together to create a single, coherent application.\n",
        "\n",
        "For example, we can create a chain that takes user input,       format it with a PromptTemplate,\n",
        "\n",
        "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by\n",
        "\n",
        "\n",
        "combining chains with other components.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lrNAo0L9CWVA"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(sequential_chain, {\"text\" : input_text, \"style\" : \"a singer\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "GTXJuXypC3gG",
        "outputId": "f2dfb234-0e87-4f7b-b474-24275c4595fb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 178 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nChains let us join together many pieces \\nTo make one application that makes sense. \\nFor instance, we can make a chain that takes user input, \\nFormats it with a PromptTemplate, and then sends it to an LLM. \\nWe can make more intricate chains by joining multiple chains, \\nOr by combining chains with other components.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcR-BmsWDGRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A note on langchain-hub\n",
        "langchain-hub is a sister library to langchain, where all the chains, agents and prompts are serialized for us to use."
      ],
      "metadata": {
        "id": "ocMuLCXBG-1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import load_chain"
      ],
      "metadata": {
        "id": "TbGZ5SUqHB94"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading from the LangChain Hub is simple. Just find the chain you want in the repository and use `load_chain` with the right path. There are also \"load_prompt\" and `initialize_agent`,\" but we'll talk about those later. For now, let's focus on loading our `LLMMathChain` that we looked at earlier."
      ],
      "metadata": {
        "id": "BLiNbo-xHPEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_math_chain = load_chain(\"lc://chains/llm-math/chain.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB9GMW0KHZZF",
        "outputId": "bd061246-818b-43ac-e735-0fbfcbad3163"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:56: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can change the parameters as well\n",
        "\n",
        "llm_math_chain.verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qCIN92tHyis",
        "outputId": "a568554a-e226-43f7-fa16-2ca5c94652c6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_math_chain.verbose = False"
      ],
      "metadata": {
        "id": "x6s_giRWH8l9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_math_chain.verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTcZ7VNXIEOu",
        "outputId": "6de8b812-7f33-46a6-a3ff-b08c513189a5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qaCJSu89IGGm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}