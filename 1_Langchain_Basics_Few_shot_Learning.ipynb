{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcaEMfh4NVT6",
        "outputId": "9d310c94-14a0-4bad-bc68-be9db283c69a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI, OpenAIChat\n",
        "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "YsDbK0gINeNe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Shot Prompt Templates\n",
        "</br>\n",
        "Langchain introduces a valuable feature known as the FewShotPromptTemplate object, designed for few-shot learning. This approach to model training enables accurate predictions with minimal examples.\n",
        "\n",
        "The main knowledge sources for Language Model Models (LLMs) include:\n",
        "\n",
        "\n",
        "\n",
        "*   **Parametric Knowledge** - Information learned by the model during its training.\n",
        "\n",
        "*   **Source Knowledge** - Information provided to the model during inference through prompts.\n",
        "\n",
        "</br>\n",
        "\n",
        "**When should Few-shot training be employed?** There are instances where we may observe that the model does not behave as anticipated, and incorporating Few-shot training can provide clarity and improve understanding for the reader.\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "---\n",
        "This notebook demonstrates the process of feeding a few training examples into the model through prompts. The model learns from these examples and applies the acquired knowledge to user inputs."
      ],
      "metadata": {
        "id": "2Rm8kR0AOBwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are an AI assistant 'Jill'.\n",
        "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
        "Here are few examples:\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "pjURYEjoN4-m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 1.0)"
      ],
      "metadata": {
        "id": "Fw1Vr97MVxdh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6nShThSV4H-",
        "outputId": "41aa1e10-f6f7-4f97-a33a-f43891bac449"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 42, of course!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this situation, we're asking for a funny response, like a joke, in exchange for our serious question. However, even when we set the creativity level (i.e., temperature = 1.0) to the highest, we still get a serious answer. To assist the model in understanding, we can provide a few examples of the kind of answers we're looking for."
      ],
      "metadata": {
        "id": "LKbuZ5eVWzAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are an AI assistant 'Jill'.\n",
        "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
        "Here are few examples:\n",
        "\n",
        "User: I love spending hours searching for misplaced keys.\n",
        "AI: Ah, the thrill of turning your home upside down just for a little adventure!\n",
        "\n",
        "User: I just adore when technology decides to stop working.\n",
        "AI: Isn't it amazing how our gadgets choose the most inconvenient times to take a little vacation?\n",
        "\n",
        "User: What is the meaning of life?\"\"\""
      ],
      "metadata": {
        "id": "jjJNXVaMWJll"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQhLGTfnbsIa",
        "outputId": "e7d94883-e114-4e05-8494-f4a71d0638d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AI: 42 — or was that the answer to the Ultimate Question of Life, the Universe, and Everything?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now get a better response, and we achieved this by using few-shot learning. We added a few examples based on what we already know.\n",
        "\n",
        "To make this work with LangChain's `FewShotPromptTemplate`, here's what we need to do:"
      ],
      "metadata": {
        "id": "OuPy0K5NcFO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"Input\": \"I love spending hours searching for misplaced keys.\",\n",
        "        \"Response\": \"Ah, the thrill of turning your home upside down just for a little adventure!\"\n",
        "    },\n",
        "    {\n",
        "       \"Input\": \"I just adore when technology decides to stop working.\",\n",
        "       \"Response\": \"Isn't it amazing how our gadgets choose the most inconvenient times to take a little vacation?\"\n",
        "\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {Input}\n",
        "AI: {Response}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yGL549uxbv1O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables= ['Input', 'Response'],\n",
        "    template = example_template\n",
        ")\n",
        "\n",
        "# Breaking above prompt into prefix and suffix\n",
        "# prefix: Instruction to model. What need to be done?\n",
        "prefix = \"\"\"You are an AI assistant 'Jill'.\n",
        "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
        "Here are few examples:\"\"\"\n",
        "\n",
        "# suffix: It's user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {Input}\n",
        "AI:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kzVgd7IjdXlz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples = examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix = prefix,\n",
        "    suffix = suffix,\n",
        "    input_variables = [\"Input\"],\n",
        "    example_separator = \"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "AxOsFvMXetak"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we'll feed user query and let see what model return from few-shot prompt\n",
        "input = \"What is the meaning of life?\"\n",
        "print(few_shot_prompt.format(Input = input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBxjoG-AfQHC",
        "outputId": "66e7dd29-88e6-4797-ab3c-8247043ff656"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI assistant 'Jill'. \n",
            "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
            "Here are few examples:\n",
            "\n",
            "\n",
            "User: I love spending hours searching for misplaced keys.\n",
            "AI: Ah, the thrill of turning your home upside down just for a little adventure!\n",
            "\n",
            "\n",
            "\n",
            "User: I just adore when technology decides to stop working.\n",
            "AI: Isn't it amazing how our gadgets choose the most inconvenient times to take a little vacation?\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate the response for the input we just need to pass this prompt to out llm\n",
        "print(llm(few_shot_prompt.format(Input = input)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeVYFNP7fvVU",
        "outputId": "73b1d1ce-a52d-4e4e-e6b8-72576a01f3fd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To enjoy the little moments, seek out adventure, and never forget to laugh!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's another good response.\n",
        "\n",
        "But, this might seem a bit complicated. Why bother with things like FewShotPromptTemplate, the examples dictionary, and so on when we can achieve the same with a simple f-string?\n",
        "\n",
        "Well, this approach is stronger and has some cool features. One of them is the ability to include or exclude examples based on the length of our question.\n",
        "\n",
        "This is actually quite important because there's a limit to how much information our prompt and generated output can contain. This limit is the maximum context window, which is just the length of our prompt plus the length of our generated text (defined by `max_tokens`).\n",
        "\n",
        "So, we need to maximize the number of examples we provide to the model for few-shot learning, making sure we don't go beyond the maximum context window or slow down processing too much.\n",
        "\n",
        "Let's see how the dynamic inclusion/exclusion of examples works. First, we need more examples:"
      ],
      "metadata": {
        "id": "Azsz_Yz3iGSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
        "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite color?\",\n",
        "        \"answer\": \"79\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite food?\",\n",
        "        \"answer\": \"Carbon based lifeforms\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"What is the best thing in the world?\",\n",
        "        \"answer\": \"The perfect pizza.\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"If you could do anything in the world what would you do?\",\n",
        "        \"answer\": \"Take over the world, of course!\"\n",
        "    }, {\n",
        "        \"query\": \"Where should I travel?\",\n",
        "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables= ['query', 'answer'],\n",
        "    template = example_template\n",
        ")"
      ],
      "metadata": {
        "id": "9FjeU7FFiNjf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then rather than using the examples list of dictionaries directly we use a `LengthBasedExampleSelector` like so:"
      ],
      "metadata": {
        "id": "6F713OmoidlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    max_length = 50 # maximum length of example could have\n",
        ")"
      ],
      "metadata": {
        "id": "SPwYeRiQiXRZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: `max_length` only counts words and not spaces of new line."
      ],
      "metadata": {
        "id": "VMp-1x_XkQJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Example of how max_length = 50 works in above prompt\"\n",
        "words = re.split('[\\n ]', text) #\n",
        "print(words, f\"\\n Total number of words are: {len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY8VawaQi9Fy",
        "outputId": "3fa62a0c-6afb-45a0-a71f-a7f85da5770c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Example', 'of', 'how', 'max_length', '=', '50', 'works', 'in', 'above', 'prompt'] \n",
            " Total number of words are: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The we use the selector to initialize s `dynamic_prompt_template`"
      ],
      "metadata": {
        "id": "X8S3qJvjN903"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"You are an AI assistant 'Jill'.\n",
        "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
        "Here are few examples:\"\"\"\n",
        "\n",
        "# suffix: It's user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "huGaQQVQQMO1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector = example_selector,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = prefix,\n",
        "    suffix = suffix,\n",
        "    input_variables = ['query'],\n",
        "    example_separator = \"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "ripOdqzlkAYF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(dynamic_prompt_template.format(query = \"How do birds fly?\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGP3p_U0OfK1",
        "outputId": "53c73a51-8787-4d17-dc4c-57d843dd1034"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Through the power of positive thinking and a few secret avian techniques.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"I am currently in India and wanted to visit few Asian and Eupropean countries in comming vacation.\n",
        "Can you suggest some? \"\"\"\n",
        "\n",
        "print(dynamic_prompt_template.format(query = query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7urkJ5PP0PE",
        "outputId": "5c8dbc9d-576f-4a22-f027-6f342e5ee75b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI assistant 'Jill'. \n",
            "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
            "Here are few examples:\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: I am currently in India and wanted to visit few Asian and Eupropean countries in comming vacation.\n",
            "Can you suggest some? \n",
            "AI:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this we've limited the number of examples being given within the prompt. If we decide this is too little we can increase the `max_length` of the `example_selector`"
      ],
      "metadata": {
        "id": "vkA1G05YSjQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    max_length = 100\n",
        ")\n",
        "\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector = example_selector,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = prefix,\n",
        "    suffix = suffix,\n",
        "    input_variables = ['query'],\n",
        "    example_separator = \"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "FL87N-B0R_FS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt_template.format(query = query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emDEyiGMTSDk",
        "outputId": "ebbe6203-8330-458e-8ce3-f7ddf5249c6b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI assistant 'Jill'. \n",
            "Your task is to generate sarcastic, creative and funny responses to user inputs.\n",
            "Here are few examples:\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: What is the weather like today?\n",
            "AI: Cloudy with a chance of memes.\n",
            "\n",
            "\n",
            "User: I am currently in India and wanted to visit few Asian and Eupropean countries in comming vacation.\n",
            "Can you suggest some? \n",
            "AI:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(dynamic_prompt_template.format(query = query)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwnc-ENcTaXA",
        "outputId": "1ad87ff6-705d-42b8-9b02-919f9d333be8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, why not! Depending on your taste, you can visit Hong Kong for the food, Singapore for the shopping, Vienna for the music, and Paris for the romance!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are just a few of the prompt tooling available in LangChain. For example, there is actually an entire other set of example selectors beyond the `LengthBasedExampleSelector`. We'll cover them in detail in upcoming notebooks, or you can read about them in the LangChain docs."
      ],
      "metadata": {
        "id": "aSlra8uMUEcf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ByMut_CMUUqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}