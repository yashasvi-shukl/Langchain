{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install vllm"
      ],
      "metadata": {
        "id": "h_2gbcb3plX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ['HF_TOKEN'] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5flloPAAabT",
        "outputId": "dbf9bd2f-3212-43fd-be29-f07f3263a66e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "model = LLM(\"selfrag/selfrag_llama2_7b\", download_dir=\"/gscratch/h2lab/akari/model_cache\", dtype=\"half\") #gpu_memory_utilization=0.9, max_model_len = 150,\n",
        "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, skip_special_tokens=False)\n",
        "\n",
        "def format_prompt(input, paragraph=None):\n",
        "  prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
        "  if paragraph is not None:\n",
        "    prompt += \"[Retrieval]<paragraph>{0}</paragraph>\".format(paragraph)\n",
        "  return prompt\n",
        "\n",
        "query_1 = \"Leave odd one out: twitter, instagram, whatsapp.\"\n",
        "query_2 = \"Can you tell me the difference between llamas and alpacas?\"\n",
        "queries = [query_1, query_2]\n",
        "\n",
        "preds = model.generate([format_prompt(query) for query in queries], sampling_params)\n",
        "for pred in preds:\n",
        "  print(\"Model prediction: {0}\".format(pred.outputs[0].text))\n",
        "# Model prediction: Twitter, Instagram, and WhatsApp are all social media platforms.[No Retrieval]WhatsApp is the odd one out because it is a messaging app, while Twitter and # Instagram are primarily used for sharing photos and videos.[Utility:5]</s> (this query doesn't require factual grounding; just skip retrieval and do normal instruction-following generation)\n",
        "# Model prediction: Sure![Retrieval]<paragraph> ... (this query requires factual grounding, call a retriever)\n",
        "\n",
        "# generate with retrieved passage\n",
        "prompt = format_prompt(\"Can you tell me the difference between llamas and alpacas?\", paragraph=\"The alpaca (Lama pacos) is a species of South American camelid mammal. It is similar to, and often confused with, the llama. Alpacas are considerably smaller than llamas, and unlike llamas, they were not bred to be working animals, but were bred specifically for their fiber.\")\n",
        "preds = model.generate([prompt], sampling_params)\n",
        "print([pred.outputs[0].text for pred in preds])\n",
        "# ['[Relevant]Alpacas are considerably smaller than llamas, and unlike llamas, they were not bred to be working animals, but were bred specifically for their fiber.[Fully supported][Utility:5]</s>']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p-5uU5VnP3j",
        "outputId": "3f692792-a122-4dcc-c5c5-60a79618a44b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 01-14 17:36:40 config.py:457] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 01-14 17:36:40 llm_engine.py:70] Initializing an LLM engine with config: model='selfrag/selfrag_llama2_7b', tokenizer='selfrag/selfrag_llama2_7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=150, download_dir='/gscratch/h2lab/akari/model_cache', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 01-14 17:38:06 llm_engine.py:275] # GPU blocks: 11, # CPU blocks: 512\n",
            "INFO 01-14 17:38:08 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 01-14 17:38:08 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
            "INFO 01-14 17:38:17 model_runner.py:547] Graph capturing finished in 9 secs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction: India is the most distinct one out of the followings: India, America, Russia, Japan, and Mumbai.[Utility:5]</s>\n",
            "Model prediction: Sure![Retrieval]<paragraph>\n",
            "\n",
            "1.2.3.4.5.6.7.8.9.10.[Utility:5]</s>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[Relevant]Alpacas are considerably smaller than llamas.[Fully supported][Utility:5]</s>']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yd2qo4WzC29c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}